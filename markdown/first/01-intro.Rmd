# Theoretical background
In this chapter, I initially explore the Bayesian brain approach, providing insights into its essential concepts and principles. Following that, I delve into the mathematical and statistical foundations of Bayes' theorem, which serves as the backbone of this perspective. Subsequently, I address the difficulties and issues arising from the application of the Bayesian approach to cognition and discuss the broader fallacies of probabilistic reasoning that influence this viewpoint.

## Explaining human cognition with Bayes' theorem: perception and decision-making
The human brain features the ability to adapt and update its beliefs about the world based on new information (Bain, 2016). Bayes' theorem offers a mathematical framework for understanding how this process might work by facilitating the calculation of the probability of different events occurring, founded on the combination of prior knowledge and new evidence.
This process of integrating new evidence with prior knowledge, known as Bayesian inference, is thought to play a pivotal role in numerous aspects of human cognition, including perception, decision-making, and learning (Rahnev, 2019). Research suggests that the brain employs Bayesian principles to process sensory information and formulate predictions about the environment (Lee & Mumford, 2003; Hasson, 2017; Sedley et al., 2016). In particular, the prefrontal cortex and the posterior parietal cortex have been associated with the integration of prior knowledge and new evidence during Bayesian inference (Herd et al., 2021).
Lee and Mumford (2003) proposed a hierarchical Bayesian model to explain the brain's processes in the visual cortex, highlighting how the brain integrates prior knowledge with sensory input to perceive the environment. Similarly, Hasson (2017) investigated the neurobiology of uncertainty and its implications for statistical learning, demonstrating that the brain employs Bayesian principles when dealing with ambiguous or uncertain information. 
As research delves deeper into the role of Bayesian principles in human cognition, our understanding of the brain's use of this theorem will continue to expand. With this foundation in place, I move forward to a specific aspect of the Bayesian brain: neuronal surprise. 

### The role of neuronal surprise in the Bayesian brain 
In their study, Qiao and colleagues (2023) showed that Bayesian surprise modulates connections between stimulus processing brain regions and control regions/networks. This modulation drives cognitive flexibility through control engagement, ultimately allowing the brain to adapt to varying control demands. This research provides valuable insights into the mechanisms underlying cognitive flexibility and the source of switch costs in cognitive tasks (Qiao et al., 2023). Additionally, Gijsen et al. (2021) investigated the cortical dynamics of the somatosensory learning system and identified neural signatures of Bayesian surprise. The study found that secondary somatosensory cortices represent confidence-corrected surprise, while indications of Bayesian surprise encoding are present in the primary somatosensory cortex. This dissociation suggests that early surprise signals may control subsequent model update rates, supporting the hypothesis that early somatosensory processing reflects Bayesian perceptual learning (Gijsen et al., 2021). Furthermore, Visalli et al. (2021) examined electroencephalographic correlates of temporal Bayesian belief updating and surprise. The study demonstrated that late positive, centro-parietally distributed event-related potentials are associated with surprise and belief updating. This evidence suggests that temporal predictions are computed using Bayesian principles and are reflected in P3 modulations, similar to other cognitive domains (Visalli et al., 2021). 
These findings highlight the significance of neuronal surprise in the Bayesian brain, showcasing its role in cognitive control, perceptual learning, and belief updating. In the next chapter, I will review the mathematical and statistical underpinnings of this perspective to provide a deeper understanding of its foundations.

## Mathematical and statistical aspects of Bayes' theorem
Bayes' theorem is a crucial mathematical concept that plays a vital role in various fields of statistics and mathematics. This theorem explains that the likelihood of an event A occurring, considering that event B has already happened, equals the probability of event B taking place, given that event A has happened, multiplied by the chance of event A happening, and then divided by the likelihood of event B happening (Bayes, 1763). Mathematically, this relationship can be demonstrated as follows:
Let A  and B  be two events in a probability space. Let \( P(A) \) and \( P(B) \) be the probabilities of events \( A \) and \( B \), respectively, and let \( P(A|B) \) be the conditional probability of event \( A \), given event \( B \). Then, according to Bayes' theorem, the relationship between these probabilities is given by:

\begin{equation}
  P(A | B) = \frac{P(B | A) \times P(A)}{P(B)}.
  (\#eq:bayestheorem)
\end{equation} 

When the unconditional probability \( P(B) \) is not known, one can use the principle of disjoint events to expand it as follows:

\begin{equation}
  P(B) = P(B | A)P(A) + P(B | \lnot A)P(\lnot A),
  (\#eq:totalprobability)
\end{equation}

where \( \lnot A \) denotes the complement of event \( A \), i.e., the event that \( A \) does not occur. Substituting this expression for \( P(B) \) into the above equation for \( P(A | B) \) gives us the alternative formulation of Bayes' theorem:

\begin{equation}
  P(A | B) = \frac{P(B | A)P(A)}{P(B | A)P(A) + P(B | \lnot A)P(\lnot A)}
  \label{eq:Alt_Bayes_Theorem}
\end{equation}

This alternative formulation is particularly useful when one does not have access to the unconditional probability  \( P(B) \) but does have information about the conditional probabilities \( P(B | A) \) and \( P(B | \lnot A) \) (Jaynes, 2003).

Bayes' theorem has been employed to tackle a variety of problems across mathematics and statistics, spanning statistical inference, machine learning, and data analysis (MacKay, 2003; Nasrabadi, 2006). It is a useful tool for updating our beliefs about the likelihood of different events based on new evidence or information and has been particularly useful in situations where the data or information being analyzed is uncertain or incomplete.
When the prior probabilities and additional information used in the calculation are accurate, Bayes' theorem can provide accurate probabilities for the event in question (see Appendix A and Appendix B). The subsequent chapter addresses problems associated with Bayes' theorem when such conditions are not fulfilled.

### Introduction to problems with the Bayesian approach
As previously stated, the reliability of the resulting probabilities can be affected when certain assumptions are not met. One crucial assumption of Bayes' theorem is that event A and event B probabilities are statistically independent (Tenenbaum et al., 2011). This means that the probability of event A happening is not impacted by the likelihood of event B happening, and vice versa. If this assumption doesn't hold, the calculations might be inaccurate (Tenenbaum et al., 2011).
Additionally, the accuracy of Bayesian computations is dependent on the presence of suitable prior probabilities. These prior probabilities embody our understanding or beliefs about how likely an event is to occur before we take into account any new evidence. If the prior probabilities are not accurately determined or do not genuinely represent the real likelihood of an event occurring, the resulting probabilities may be less dependable (Jaynes, 2003). It is critical to ascertain that all conditions of Bayes' theorem are met to achieve a reliable probability. If any of these conditions are unmet, the resulting probability may also be incorrect. While these are just a few of the challenges and limitations linked to the Bayesian approach, it is important to take them into consideration when applying this method to more complex situations (see, e.g., Perfors et al., 2011). Keeping these challenges in mind, upcoming chapters will delve into the wider context of probabilistic reasoning, which entails making judgments about uncertain events or outcomes based on probabilities.
It is important to note that, due to the scope of this paper, I will not delve deeper into the problems of Bayes' theorem in mathematical and statistical contexts. See Pourret and colleagues (2018) for a more comprehensive analysis.