[["index.html", "Probabilistic fallacies in the Bayesian brain – A critique of the Bayesian approach to cognition Abstract", " Probabilistic fallacies in the Bayesian brain – A critique of the Bayesian approach to cognition Clievins Selva 2023-10-17 Abstract The Bayesian Brain hypothesis is an influential idea in cognitive neuroscience, helping to explain perception, decision-making, and learning. However, it faces obstacles related to falsifiability and conflicts with several fallacies. In this paper, I assess the transparency of Bayesian methods in mathematics and statistics against the Bayesian Brain hypothesis, the limits of intuitive probability, and possible issues with expertise in probabilistic reasoning. The analysis highlights the need to recognize the constraints and difficulties of the Bayesian approach and to investigate alternative ways of understanding cognitive processes. I argue for a thoughtful examination of the Bayesian approach’s limitations and a move towards a more mindful use of statistics. In doing this, I aim to promote a deeper and more critical view of the Bayesian Brain hypothesis and its influence on cognitive neuroscience. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction An emerging area of interest within cognitive science is the Bayesian approach to cognition, which posits that the brain processes information using probabilistic models and makes decisions based on Bayesian inference (Friston, 2010). This approach suggests that the brain continually updates its beliefs based on statistical principles and new evidence to make predictions about the likelihood of different outcomes (Denève &amp; Jardri, 2016). However, despite its widespread acceptance, the Bayesian approach to cognition is not without criticisms and limitations (Jones &amp; Love, 2021). The importance of examining the Bayesian approach to cognition lies in its far-reaching implications for neuroscience and psychology. A deeper understanding of this approach and its limitations can contribute to developing more accurate models of human cognition and decision-making, benefiting these interdisciplinary fields. This paper adds value by critically analyzing the theoretical foundations of the Bayesian approach to cognition, covering aspects such as Bayes’ theorem, neuronal surprise, probabilistic reasoning, falsifiability, transparency, and intuitive probability. The research question that this paper aims to address is: “To what extent do probabilistic fallacies affect the reliability and validity of the Bayesian approach to cognition?” Finally, I will discuss the key findings of my analysis, interpret their implications, and provide suggestions for future research directions in my concluding remarks. "],["theoretical-background.html", "Chapter 2 Theoretical background 2.1 Explaining human cognition with Bayes’ theorem: perception and decision-making 2.2 Mathematical and statistical aspects of Bayes’ theorem", " Chapter 2 Theoretical background In this chapter, I initially explore the Bayesian brain approach, providing insights into its essential concepts and principles. Following that, I delve into the mathematical and statistical foundations of Bayes’ theorem, which serves as the backbone of this perspective. Subsequently, I address the difficulties and issues arising from the application of the Bayesian approach to cognition and discuss the broader fallacies of probabilistic reasoning that influence this viewpoint. 2.1 Explaining human cognition with Bayes’ theorem: perception and decision-making The human brain features the ability to adapt and update its beliefs about the world based on new information (Bain, 2016). Bayes’ theorem offers a mathematical framework for understanding how this process might work by facilitating the calculation of the probability of different events occurring, founded on the combination of prior knowledge and new evidence. This process of integrating new evidence with prior knowledge, known as Bayesian inference, is thought to play a pivotal role in numerous aspects of human cognition, including perception, decision-making, and learning (Rahnev, 2019). Research suggests that the brain employs Bayesian principles to process sensory information and formulate predictions about the environment (Lee &amp; Mumford, 2003; Hasson, 2017; Sedley et al., 2016). In particular, the prefrontal cortex and the posterior parietal cortex have been associated with the integration of prior knowledge and new evidence during Bayesian inference (Herd et al., 2021). Lee and Mumford (2003) proposed a hierarchical Bayesian model to explain the brain’s processes in the visual cortex, highlighting how the brain integrates prior knowledge with sensory input to perceive the environment. Similarly, Hasson (2017) investigated the neurobiology of uncertainty and its implications for statistical learning, demonstrating that the brain employs Bayesian principles when dealing with ambiguous or uncertain information. As research delves deeper into the role of Bayesian principles in human cognition, our understanding of the brain’s use of this theorem will continue to expand. With this foundation in place, I move forward to a specific aspect of the Bayesian brain: neuronal surprise. 2.1.1 The role of neuronal surprise in the Bayesian brain In their study, Qiao and colleagues (2023) showed that Bayesian surprise modulates connections between stimulus processing brain regions and control regions/networks. This modulation drives cognitive flexibility through control engagement, ultimately allowing the brain to adapt to varying control demands. This research provides valuable insights into the mechanisms underlying cognitive flexibility and the source of switch costs in cognitive tasks (Qiao et al., 2023). Additionally, Gijsen et al. (2021) investigated the cortical dynamics of the somatosensory learning system and identified neural signatures of Bayesian surprise. The study found that secondary somatosensory cortices represent confidence-corrected surprise, while indications of Bayesian surprise encoding are present in the primary somatosensory cortex. This dissociation suggests that early surprise signals may control subsequent model update rates, supporting the hypothesis that early somatosensory processing reflects Bayesian perceptual learning (Gijsen et al., 2021). Furthermore, Visalli et al. (2021) examined electroencephalographic correlates of temporal Bayesian belief updating and surprise. The study demonstrated that late positive, centro-parietally distributed event-related potentials are associated with surprise and belief updating. This evidence suggests that temporal predictions are computed using Bayesian principles and are reflected in P3 modulations, similar to other cognitive domains (Visalli et al., 2021). These findings highlight the significance of neuronal surprise in the Bayesian brain, showcasing its role in cognitive control, perceptual learning, and belief updating. In the next chapter, I will review the mathematical and statistical underpinnings of this perspective to provide a deeper understanding of its foundations. 2.2 Mathematical and statistical aspects of Bayes’ theorem Bayes’ theorem is a crucial mathematical concept that plays a vital role in various fields of statistics and mathematics. This theorem explains that the likelihood of an event A occurring, considering that event B has already happened, equals the probability of event B taking place, given that event A has happened, multiplied by the chance of event A happening, and then divided by the likelihood of event B happening (Bayes, 1763). Mathematically, this relationship can be demonstrated as follows: Let A and B be two events in a probability space. Let \\(P(A)\\) and \\(P(B)\\) be the probabilities of events \\(A\\) and \\(B\\), respectively, and let \\(P(A|B)\\) be the conditional probability of event \\(A\\), given event \\(B\\). Then, according to Bayes’ theorem, the relationship between these probabilities is given by: \\[\\begin{equation} P(A | B) = \\frac{P(B | A) \\times P(A)}{P(B)}. \\tag{2.1} \\end{equation}\\] When the unconditional probability \\(P(B)\\) is not known, one can use the principle of disjoint events to expand it as follows: \\[\\begin{equation} P(B) = P(B | A)P(A) + P(B | \\lnot A)P(\\lnot A), \\tag{2.2} \\end{equation}\\] where \\(\\lnot A\\) denotes the complement of event \\(A\\), i.e., the event that \\(A\\) does not occur. Substituting this expression for \\(P(B)\\) into the above equation for \\(P(A | B)\\) gives us the alternative formulation of Bayes’ theorem: \\[\\begin{equation} P(A | B) = \\frac{P(B | A)P(A)}{P(B | A)P(A) + P(B | \\lnot A)P(\\lnot A)} \\label{eq:Alt_Bayes_Theorem} \\end{equation}\\] This alternative formulation is particularly useful when one does not have access to the unconditional probability \\(P(B)\\) but does have information about the conditional probabilities \\(P(B | A)\\) and \\(P(B | \\lnot A)\\) (Jaynes, 2003). Bayes’ theorem has been employed to tackle a variety of problems across mathematics and statistics, spanning statistical inference, machine learning, and data analysis (MacKay, 2003; Nasrabadi, 2006). It is a useful tool for updating our beliefs about the likelihood of different events based on new evidence or information and has been particularly useful in situations where the data or information being analyzed is uncertain or incomplete. When the prior probabilities and additional information used in the calculation are accurate, Bayes’ theorem can provide accurate probabilities for the event in question (see Appendix A and Appendix B). The subsequent chapter addresses problems associated with Bayes’ theorem when such conditions are not fulfilled. 2.2.1 Introduction to problems with the Bayesian approach As previously stated, the reliability of the resulting probabilities can be affected when certain assumptions are not met. One crucial assumption of Bayes’ theorem is that event A and event B probabilities are statistically independent (Tenenbaum et al., 2011). This means that the probability of event A happening is not impacted by the likelihood of event B happening, and vice versa. If this assumption doesn’t hold, the calculations might be inaccurate (Tenenbaum et al., 2011). Additionally, the accuracy of Bayesian computations is dependent on the presence of suitable prior probabilities. These prior probabilities embody our understanding or beliefs about how likely an event is to occur before we take into account any new evidence. If the prior probabilities are not accurately determined or do not genuinely represent the real likelihood of an event occurring, the resulting probabilities may be less dependable (Jaynes, 2003). It is critical to ascertain that all conditions of Bayes’ theorem are met to achieve a reliable probability. If any of these conditions are unmet, the resulting probability may also be incorrect. While these are just a few of the challenges and limitations linked to the Bayesian approach, it is important to take them into consideration when applying this method to more complex situations (see, e.g., Perfors et al., 2011). Keeping these challenges in mind, upcoming chapters will delve into the wider context of probabilistic reasoning, which entails making judgments about uncertain events or outcomes based on probabilities. It is important to note that, due to the scope of this paper, I will not delve deeper into the problems of Bayes’ theorem in mathematical and statistical contexts. See Pourret and colleagues (2018) for a more comprehensive analysis. "],["the-problems-of-probabilistic-reasoning.html", "Chapter 3 The problems of probabilistic reasoning 3.1 Berkson’s paradox 3.2 Inverse fallacy 3.3 Prosecutor’s fallacy 3.4 Monty Hall problem 3.5 Gambler’s fallacy", " Chapter 3 The problems of probabilistic reasoning Probabilistic reasoning refers to the act of making judgments about uncertain events or outcomes using probabilities. Erwig and Walkingshaw (2009) emphasize that probabilistic reasoning can be difficult for individuals lacking formal education in mathematics or statistics. They further argue that even basic questions concerning conditional probabilities can be perplexing for those without the appropriate background. Subsequently, fallacies connected to this type of reasoning will be explained. 3.1 Berkson’s paradox Berkson’s paradox is a specific type of statistical bias that arises when conclusions about a population are drawn from a sample that does not accurately represent the entire population (Berkson, 1946). To illustrate Berkson’s paradox, let’s consider a hypothetical example involving the correlation between two health-related factors, A and B, which are independent in the entire population. Suppose that factor A is the presence of a specific gene, and factor B is a particular lifestyle habit. In the general population, there is no correlation between possessing the gene and engaging in the lifestyle habit. However, when examining a subset of the population that is affected by a specific health condition, one might find a negative correlation between the presence of the gene and the lifestyle habit, even though there is no correlation in the general population. This occurs because the health condition acts as a confounding variable, and the examined sample is not representative of the whole population. When applied to the Bayesian brain hypothesis, Berkson’s paradox may manifest in situations where the brain relies on non-representative samples to make decisions or predictions, leading to biased inferences (Griffiths et al., 2008). For instance, an individual may have observed a small sample of events or interactions that are not representative of the overall population, and based on this limited information, their brain forms a biased belief or expectation about future occurrences. This can occur in various cognitive processes, such as social perception (Kahneman &amp; Tversky, 1973), decision-making (Tversky &amp; Kahneman, 1981), and learning (Gershman, 2015), where individuals are likely to encounter non-representative samples due to selective attention, memory biases, or limited exposure. Empirical evidence supporting the occurrence of this fallacy in cognition includes studies demonstrating biased judgments in probability estimation tasks (Tversky &amp; Kahneman, 1974) and suboptimal decision-making in various contexts (Daw et al., 2005). 3.2 Inverse fallacy The inverse fallacy involves erroneously believing that the reverse of a conditional probability is the same as the conditional probability of the inverse event (Dahlman et al., 2016). As shown in Appendix B, this fallacy can cause someone to incorrectly assume that if \\(P(B|A)\\) is 0.7 (the probability the forecast predicts rain given that it will rain), then \\(P(A|B)\\) must be 1 - 0.7 = 0.3 (the probability that it will rain given the forecast predicts rain). In reality, one must apply Bayes’ theorem to determine the correct probability, which is approximately 81.4% for\\(P(A|B)\\) , rather than the incorrect assumption of 30%. An empirical study by Casscells et al. (1978) investigated the occurrence of the inverse fallacy in cognition. They examined physicians’ comprehension of conditional probabilities in medical diagnosis. The results revealed that a mere 18% of physicians provided the correct answer, while the majority fell prey to the inverse fallacy. This supports the notion that the inverse fallacy can hinder the accurate application of Bayesian reasoning principles, even among medical professionals. 3.3 Prosecutor’s fallacy The prosecutor’s fallacy is a type of error that occurs when one assumes that the presence of a piece of evidence increases the probability of a hypothesis being true without taking into account the prior probability of the hypothesis and the probability of observing the evidence given the truth of the hypothesis. This fallacy is frequently encountered in legal situations, where the likelihood of a defendant’s guilt may be overstated based on evidence, such as a DNA match, without considering the relevant probabilities (Thompson &amp; Schumann, 1987). Take, for example, a situation in which a fingerprint found at a crime scene matches that of a suspect. If the likelihood of a random fingerprint matching the suspect is 1 in 10,000, it might be tempting to assume the suspect is guilty, with a probability of 99.99%. However, this line of reasoning neglects the prior probability of the suspect’s guilt and the likelihood of observing the fingerprint evidence, given the truth of the hypothesis. The prosecutor’s fallacy arises when these probabilities are not considered, leading to inflated estimations of a suspect’s guilt based solely on the evidence. Within the realm of cognition and the Bayesian brain, the prosecutor’s fallacy may emerge when individuals overestimate the likelihood of a certain outcome or hypothesis based on a single piece of evidence, neglecting to consider the relevant probabilities (Leung, 2002). This fallacy has the potential to introduce bias into judgments and decision-making across various cognitive processes, such as reasoning, learning, and perception. A study conducted by Thompson and Schumann (1987) found that participants were more prone to the prosecutor’s fallacy when presented with evidence framed in a way that emphasized its uniqueness without underscoring the importance of considering prior probabilities and the likelihood of the evidence given the truth of the hypothesis. 3.4 Monty Hall problem The Monty Hall problem is a probability puzzle that is based on the television game show “Let’s Make a Deal” (Chen &amp; Wang, 2020). In the game show scenario, a contestant is presented with three doors, with one of them hiding a valuable prize (e.g., a car) and the other two concealing less valuable prizes (e.g., goats). The contestant selects one door, and the host, Monty Hall, reveals one of the unchosen doors without the valuable prize. At this point, the contestant has the option to either stick with their initial choice or switch to the other unopened door. The question arises: does the contestant have better odds of winning the valuable prize by switching doors or by staying with their initial selection? Many people initially believe that the contestant’s chances are 50-50, regardless of whether they switch or stick with their original choice. However, the correct answer is that the contestant has a higher chance of winning the prize ( or 66.7%) if they switch doors, compared to a or 33.3% chance if they stick with their original choice. The Monty Hall problem contributes to the Bayesian brain discussion by demonstrating the impact of information presentation on individuals’ probability estimations. It reveals that people frequently fail to consider broader information, focusing on specific details instead. This indicates that the brain might not consistently process information following Bayesian principles, which in turn affects probability judgments. In a study by Krauss and Wang (2003), participants faced the Monty Hall problem and demonstrated a persistent preference for non-optimal strategies, indicating cognitive biases at play. This evidence supports the notion that the brain’s ability to apply Bayesian reasoning might be limited, highlighting the importance of understanding such biases in the context of human cognition. 3.5 Gambler’s fallacy The gambler’s fallacy is a cognitive bias that occurs when people mistakenly believe that the probability of a random event is influenced by previous outcomes. For example, imagine a person tossing a fair coin and getting five consecutive heads. They might believe that the next toss is more likely to result in tails because ‘it’s due.’ This belief is fallacious because the outcome of each coin toss is independent of previous outcomes. In a fair coin toss, the probability of getting heads or tails is always 50%, regardless of the sequence of previous outcomes. Empirical evidence supporting the occurrence of the gambler’s fallacy in cognition includes studies by Croson and Sundali (2005), who investigated the fallacy in casino settings. They found that players tended to bet more on the opposite color in roulette after observing a streak of the same color, demonstrating the gambler’s fallacy in action. Another study by Roney and Trick (2009) examined the gambler’s fallacy in sports. They found that basketball players and fans exhibited the fallacy when predicting the outcomes of free throws. Participants believed that a player who had made several successful throws in a row was more likely to miss the next one, despite the fact that each throw is an independent event. As this and the previous sections have illustrated, prediction is difficult when it is conflicted with fallacies (Neth &amp; Gigerenzer, 2015). With that being said, I will further evaluate the falsifiability of the Bayesian brain hypotheses and additionally include certain aspects (i.e., transparency, limits, and pitfalls) in the following chapter. "],["the-falsifiability-of-the-bayesian-brain-hypothesis.html", "Chapter 4 The falsifiability of the Bayesian brain hypothesis 4.1 Transparency of Bayesian approaches in mathematics and statistics vs. the Bayesian brain hypothesis 4.2 The limits of intuitive probability 4.3 Probability and the pitfalls of expertise", " Chapter 4 The falsifiability of the Bayesian brain hypothesis One key challenge faced by the Bayesian brain hypothesis is its potential lack of falsifiability. Falsifiability, as defined by Popper (1959), pertains to a scientific theory’s ability to be empirically tested and potentially refuted. The Bayesian brain hypothesis, however, is not easily testable or refutable, which raises concerns regarding its status as a scientific theory (Bowers &amp; Davis, 2012, as cited in Griffiths et al., 2012). Firstly, the hypothesis is built upon probabilistic models and statistical inference, making it inherently complex and challenging to verify or test empirically (Friston, 2010; Wiese, 2014). Due to the probabilistic nature of Bayesian inference, it is difficult to derive clear-cut predictions to test experimentally. Even if a specific prediction is generated, an observed deviation from the prediction may not necessarily falsify the hypothesis, as it could be attributed to other factors or noise in the data (Bowers &amp; Davis, 2012, cited in Griffiths et al., 2012). Secondly, the hypothesis employs abstract concepts, such as priors and free energy (Friston, 2010; Friston et al., 2006), which are challenging to operationalize and measure rigorously. Despite the wealth of research supporting the Bayesian brain hypothesis and its utility in understanding cognitive processes (Kersten et al., 2004), there is a need for more concrete evidence demonstrating that the brain employs Bayesian principles to process and integrate sensory information. Friston (2010) acknowledges this challenge and links the Bayesian brain hypothesis to the theory of natural selection, which, although not strictly falsifiable, has proven to be a powerful explanatory framework. In subsequent sections, I will examine some aspects that are not directly connected to the Bayesian brain hypothesis. Nonetheless, these critical aspects influence the discussion and should be considered when applying Bayes’ theorem to cognition. 4.1 Transparency of Bayesian approaches in mathematics and statistics vs. the Bayesian brain hypothesis In the fields of mathematics and statistics, transparency is essential for understanding and evaluating the assumptions and logic behind models (Gelman et al., 2013). Bayesian methods in these disciplines allow researchers to pinpoint inaccurate priors or incorrect calculations at any stage. For instance, consider the calculations from Appendix B: if the posterior probability appears to be incorrect, both the practitioner and the observer(s) can identify the root of the problem (e.g., a poorly chosen prior) independently. This transparency is particularly beneficial when updating priors with new information, leading to more precise probability estimates (Gelman et al., 2013). However, the Bayesian brain hypothesis does not provide the same level of transparency. Despite advancements in neuroimaging techniques like functional magnetic resonance imaging (fMRI) and EEG, researchers can only examine the internal representation of a Bayesian mind, not the appropriateness of its application (Friston, 2010). This constraint makes it difficult to evaluate the model’s reliability and validity and raises questions about the actual neural mechanisms underlying the Bayesian brain hypothesis (Friston, 2010). 4.2 The limits of intuitive probability In Appendix B, I calculated an 81.4% probability of rain on a given day. Gigerenzer et al. (2005) investigated people’s understanding of such probabilities, asking participants from five major cities to rank three interpretations of a 30% chance of rain tomorrow. The correct interpretation (that it will rain on 30% of the days like tomorrow) was only the most popular interpretation in New York. In the European cities, the preferred interpretation was that it would rain 30% of the time or in 30% of the area. All European samples picked the correct interpretation as the least appropriate one. Even though one must consider moderating effects (i.e., different degrees of exposure to probabilistic forecasts) when interpreting the study, however, it does indicate the limits of intuitiveness in day-to-day probabilities (see also Murphy et al., 1980; Handmer &amp; Proudley, 2007). Based on their results, Gigerenzer and colleagues (2005) advise experts to specify the reference class or the class of events to which a single-event probability refers. Given that, one can conclude that probabilities must be specifically learned, and a certain degree of expertise is needed to handle probabilities appropriately, which is also highlighted in Erwig and Walkingshaw (2009). Therefore, one might assume that experts in this field (e.g., mathematicians and statisticians) are immune to the biases presented within this paper. However, in the following section, I discuss empirical data that suggests that even experts in their field of expertise struggle with probabilities and their interpretations. 4.3 Probability and the pitfalls of expertise As highlighted in previous sections, probabilistic reasoning can be challenging for individuals without a formal education in mathematics or statistics (Erwig &amp; Walkingshaw, 2019). However, one can argue that even for experts in mathematics and statistics, probability interpretation can be a complex and daunting task. Studies by Gigerenzer (2004) and Hoekstra et al. (2014) demonstrated that both mathematicians and statistics professors made errors in interpreting p-values and confidence intervals. Given that, expertise alone does not guarantee immunity from the pitfalls associated with probabilistic reasoning. In combination with the lack of transparency in the Bayesian brain hypothesis (Chapter 4.1), the unintuitive nature of probabilities (Chapter 4.2), and the susceptibility of experts to errors, all contribute to the complexity of probability interpretation. "],["discussion.html", "Chapter 5 Discussion 5.1 Interpretation of findings 5.2 Limitations and future directions for research", " Chapter 5 Discussion In this discussion, I will summarize my findings and interpret them concerning my research question. My study aimed to explore the impact of Bayesian reasoning on the understanding of human cognition and the limitations it presents. The results have provided valuable insights. I will discuss the patterns, unexpected results, and limitations. I will also suggest future research directions in this area. 5.1 Interpretation of findings The Bayesian brain hypothesis has significantly influenced cognitive science, contributing to advancements in understanding perception, decision-making, and learning. Despite its impact, the hypothesis is not a complete explanation for all aspects of human cognition. It is important to consider the influence of fallacies and irrational thinking on human cognition, as Williams (2021) highlights the role of epistemic irrationality in the Bayesian brain, arguing that the hypothesis may not fully account for the complexities of human cognition. This raises concerns about the reliability of the Bayesian approach as a comprehensive model for understanding cognition. As McKenzie (2003) states, “it is suggested here that rational models be seen as theories, and not standards, of behavior” (p. 403). Considering the Bayesian brain hypothesis as a theoretical framework rather than an infallible standard allows for a more nuanced understanding of human cognition, taking into account the impact of fallacies and irrational thinking. Regarding falsifiability, Friston (2010) asserts that it is not falsifiable in the same sense that natural selection is not falsifiable. However, the framework becomes very useful for generating and testing models of cognition. The Bayesian brain hypothesis has proven to be a valuable framework for understanding brain processes. Nonetheless, this necessitates further exploration and improvement of the theory based on emerging evidence and research advancements. If we accept the premise that humans have a Bayesian brain, we need to consider whether the related fallacies and limitations could hinder our ability to use this cognitive system effectively. A reader might assume that certain fallacies are widely known or that knowledge of them is sufficient to avoid committing them. However, I argue that once a fallacy unfolds in the brain, identifying it becomes much more difficult. In a mathematical or statistical context, errors can be overlooked, but this does not apply to cognition. People tend to quickly generate explanations and make assumptions without realizing their origin, which can lead to the adoption of fallacies (Gigerenzer, 2020; Neth &amp; Gigerenzer, 2015). As a result, one might not commit certain fallacies when conducting statistical analysis but is still susceptible to them when engaging in cognition. 5.2 Limitations and future directions for research In this paper, I have discussed the limitations of the Bayesian brain hypothesis and highlighted various fallacies impacting its applicability. However, my analysis has limitations warranting further exploration. Firstly, the multi-intercorrelation of fallacies was not thoroughly investigated, as they contribute differently in various contexts and are influenced by the combination of fallacies present. Secondly, this paper presents only a selected number of fallacies, and therefore, not all the fallacies that might be relevant to the topic are discussed. Finally, only a nuanced description of the Bayesian Brain approach to cognition is provided, without delving into closely related concepts, such as free energy (Friston, 2010). Future research should focus on improving the transparency of the Bayesian brain hypothesis and exploring and combining alternative frameworks to better understand the brain’s underlying cognitive processes. This may involve further examining the relationship between neural activity and Bayesian computations (Aitchison &amp; Lengyel, 2017), determining whether neural activity patterns can be directly linked to the Bayesian computations or if alternative explanations, such as other cognitive processes, can account for the observed neural activity. Additionally, researchers could consider alternative frameworks, such as predictive coding (Clark, 2013), which emphasizes the brain’s ability to generate and update internal models of the world based on sensory input, minimizing prediction errors through a hierarchical structure (Friston, 2010). This approach may provide a more transparent link between neural activity and cognitive processes compared to the Bayesian brain hypothesis. A combination of these or other computational models in the future might control for some of the fallacies discussed in this paper and potentially lead to more accurate predictions of human cognition. "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion The central question explored in this paper is, “To what extent do probabilistic fallacies affect the reliability and validity of the Bayesian approach to cognition?” I have discussed various criticisms and limitations of the Bayesian brain hypothesis, stressing that the answer is more intricate than a simple affirmative or negative. Recognizing the limitations associated with the Bayesian approach and exploring alternative frameworks is vital for a comprehensive and nuanced understanding of human cognition (Griffiths et al., 2015; Griffiths et al., 2010; Tenenbaum et al., 2011). Gigerenzer’s experiment and others reveal that even experts grapple with identifying correct probabilities, indicating that understanding probability is complex for humans. Moreover, cognitive and emotional biases may impede the brain’s ability to use Bayesian inference, leading to systematic errors in judgment. Consequently, it is crucial to approach the Bayesian brain hypothesis with humility and openness. In this regard, Cohen’s critique of null hypothesis significance testing (NHST – a frequentist probability procedure) is, ironically, appliable to the epistemic (i.e., Bayes) approach to cognition: What’s wrong with NHST? Well, among other things, it does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does (Cohen, 1994, p. 997). In conclusion, I advocate for a paradigm shift towards a more mindful (i.e., Gigerenzer, 2004) application of the Bayesian approach to cognition (see also Tukey, 1962; Taleb, 2008). This is not to dismiss studies supporting the Bayesian brain hypothesis but to acknowledge the challenges in evaluating the Bayesian approach, given our current methodologies and understanding of cognition. By addressing these limitations and further integrating Bayesian cognitive models with other models, we can develop a more accurate and holistic perspective on the human mind’s intricacies. "],["references.html", "References", " References Aitchison, L., &amp; Lengyel, M. (2017). With or without you: predictive coding and Bayesian inference in the brain. Current Opinion in Neurobiology, 46, 219-227. https://doi.org/10.1016/j.conb.2017.08.010 Bain, R. (2016). Are our brains Bayesian? Significance, 13(4), 14-19. https://doi.org/10.1111/j.1740-9713.2016.00935.x Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London, 53, 370-418. Berkson, J. (1946). Limitations of the Application of Fourfold Table Analysis to Hospital Data. Biometrics Bulletin, 2(3), 47-53. https://doi.org/10.1093/ije/dyu022 Casscells, W., Schoenberger, A., &amp; Graboys, T. B. (1978). Interpretation by physicians of clinical laboratory results. New England Journal of Medicine, 299(18), 999-1001. https://doi.org/10.1056/NEJM197811022991808 Chen, W. J., &amp; Wang, J. T. (2020). A modified Monty Hall problem. Theory and Decision, 89, 151-156. https://doi.org/10.1007/s11238-020-09757-1 Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral and Brain Sciences, 36(3), 181-204. https://doi.org/10.1017/S0140525X12000477 Cohen, J. (1994). The earth is round (p &lt; .05). American Psychologist, 49(12), 997–1003. https://doi.org/10.1037/0003-066X.49.12.997 Croson, R., &amp; Sundali, J. (2005). The gambler’s fallacy and the hot hand: Empirical data from casinos. Journal of Risk and Uncertainty, 30(3), 195-209. https://doi.org/10.1007/s11166-005-1153-2 Dahlman, C., Zenker, F., &amp; Sarwar, F. (2016). Miss rate neglect in legal evidence. Law, Probability and Risk, 15(4), 239-250. https://doi.org/10.1093/lpr/mgw007 Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., &amp; Dolan, R. J. (2005). Cortical substrates for exploratory decisions in humans. Nature, 441(7095), 876-879. https://doi.org/10.1038/nature04766 Denève, S., &amp; Jardri, R. (2016). Circular inference: mistaken belief, misplaced trust. Current Opinion in Behavioral Sciences, 11, 40-48. https://doi.org/10.1016/j.cobeha.2016.04.001 Doelling, K. B., &amp; Assaneo, M. F. (2021). Neural oscillations are a start toward understanding brain activity rather than the end. PLoS Biology, 19(5), e3001234. https://doi.org/10.1371/journal.pbio.3001234 Erwig, M., &amp; Walkingshaw, E. (2009). A DSL for explaining probabilistic reasoning. In IFIP Working Conference on Domain-Specific Languages, 335-359. https://doi.org/10.1007/978-3-642-03034-5_16 Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2), 127-138. https://doi.org/10.1038/nrn2787 Friston, K., Kilner, J., &amp; Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology-Paris, 100(1-3), 70-87. https://doi.org/10.1016/j.jphysparis.2006.10.001 Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). Chapman &amp; Hall/CRC. https://doi.org/10.1201/b16018 Gershman, S. J. (2015). Do learning rates adapt to the distribution of rewards? Psychonomic Bulletin &amp; Review, 22(5), 1320-1327. https://doi.org/10.3758/s13423-014-0790-3 Gigerenzer, G. (2004). Mindless statistics. Journal of Socio-Economics, 33(5), 587-606. https://doi.org/10.1016/j.socec.2004.09.033 Gigerenzer, G. (2020). How to explain behavior? Topics in Cognitive Science, 12(4), 1363-1381. https://doi.org/10.1111/tops.12480 Gigerenzer, G., Hertwig, R., Van Den Broek, E., Fasolo, B., &amp; Katsikopoulos, K. V. (2005). “A 30% chance of rain tomorrow”: How does the public understand probabilistic weather forecasts? Risk Analysis: An International Journal, 25(3), 623-629. https://doi.org/10.1111/j.1539-6924.2005.00608.x Gijsen, S., Grundei, M., Lange, R. T., Ostwald, D., &amp; Blankenburg, F. (2021). Neural surprise in somatosensory Bayesian learning. PLoS Computational Biology, 17(2). https://doi.org/10.1371/journal.pcbi.1008068 Griffiths, T. L., Chater, N., Kemp, C., Perfors, A., &amp; Tenenbaum, J. B. (2008). Probabilistic models of cognition: exploring representations and inductive biases. Trends in Cognitive Sciences, 14(8), 357-364. https://doi.org/10.1016/j.tics.2010.05.004 Griffiths, T. L., Chater, N., Norris, D., &amp; Pouget, A. (2012). How the Bayesians got their beliefs (and what those beliefs actually are): Comment on Bowers and Davis (2012). https://doi.org/10.1037/a0026884 Handmer, J., &amp; Proudley, B. (2007). Communicating uncertainty via probabilities: The case of weather forecasts. Environmental Hazards, 7(2), 79-87. https://doi.org/10.1016/j.envhaz.2007.05.002 Hasson, U. (2017). The neurobiology of uncertainty: Implications for statistical learning. Philosophical Transactions of the Royal Society B: Biological Sciences, 372(1711), 20160048. https://doi.org/10.1098/rstb.2016.0048 Herd, S., Krueger, K., Nair, A., Mollick, J., &amp; O’Reilly, R. (2021). Neural mechanisms of human decision-making. Cognitive, Affective, &amp; Behavioral Neuroscience, 21(1), 35-57. https://doi.org/10.3758/s13415-020-00842-0 Hoekstra, R., Morey, R. D., Rouder, J. N., &amp; Wagenmakers, E. J. (2014). Robust misinterpretation of confidence intervals. Psychonomic Bulletin &amp; Review, 21, 1157-1164. https://doi.org/10.3758/s13423-013-0572-3 Jaynes, E. T. (2003). Probability theory: The logic of science. Cambridge University Press. https://doi.org/10.1017/CBO9780511790423 Jones, M., &amp; Love, B. C. (2021). Bayesian fundamentalism or enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition. Behavioral and Brain Sciences, 34(4), 169-188. https://doi.org/10.1017/S0140525X10003134 Kahneman, D., &amp; Tversky, A. (1973). On the psychology of prediction. Psychological Review, 80(4), 237-251. https://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0034747 Kersten, D., Mamassian, P., &amp; Yuille, A. (2004). Object perception as Bayesian inference. Annual Review of Psychology, 55, 271-304. https://doi.org/10.1146/annurev.psych.55.090902.142005 Krauss, S., &amp; Wang, X. T. (2003). The psychology of the Monty Hall problem: Discovering psychological mechanisms for solving a tenacious brain teaser. Journal of Experimental Psychology: General, 132(1), 3-22. https://doi.org/10.1037/0096-3445.132.1.3 Lee, T. S., &amp; Mumford, D. (2003). Hierarchical Bayesian inference in the visual cortex. Journal of the Optical Society of America, A, Optics, Image Science &amp; Vision, 20(7), 1434–1448. https://doi.org/10.1364/JOSAA.20.001434 Leung, W. C. (2002). The prosecutor’s fallacy—A pitfall in interpreting probabilities in forensic evidence. Medicine, Science and the Law, 42(1), 44-50. https://doi.org/10.1177/002580240204200108 MacKay, D. J. (2003). Information theory, inference and learning algorithms. Cambridge University Press. https://openlibrary.org/books/OL22584006M McKenzie, C. R. (2003). Rational models as theories–not standards–of behavior. Trends in Cognitive Sciences, 7(9), 403-406. https://doi.org/10.1016/S1364-6613(03)00196-7 Murphy, A. H., Lichtenstein, S., Fischhoff, B., &amp; Winkler, R. L. (1980). Misinterpretations of precipitation probability forecasts. Bulletin of the American Meteorological Society, 61(7), 695-701. https://doi.org/10.1175/1520-0477(1980)061&lt;0695:MOPPF&gt;2.0.CO;2 Nasrabadi, N. M. (2007). Pattern recognition and machine learning. Journal of Electronic Imaging, 16(4), 049901. https://doi.org/10.1117/1.2819119 Neth, H., &amp; Gigerenzer, G. (2015). Heuristics: Tools for an uncertain world. In Emerging trends in the social and behavioral sciences, 1-18. https://doi.org/10.1002/9781118900772.etrds0394 Perfors, A., Tenenbaum, J. B., Griffiths, T. L., &amp; Xu, F. (2011). A tutorial introduction to Bayesian models of cognitive development. Cognition, 120(3), 302-321. https://doi.org/10.1016/j.cognition.2010.11.015 Popper, K. (1959). The Logic of Scientific Discovery. Hutchinson &amp; Co. https://doi.org/10.4324/9780203994627 Pourret, O., Na, P., &amp; Marcot, B. (Eds.). (2008). Bayesian networks: a practical guide to applications. John Wiley &amp; Sons. https://doi.org/10.1002/9780470994559 Qiao, L., Zhang, L., &amp; Chen, A. (2023). Brain connectivity modulation by Bayesian surprise in relation to control demand drives cognitive flexibility via control engagement. Cerebral Cortex, 33(5), 1985-2000. https://doi.org/10.1093/cercor/bhac187 Rahnev, D. (2019). The Bayesian brain: What is it and do humans have it? Behavioral and Brain Sciences, 42. https://doi.org/10.1017/S0140525X19001377 Roney, C. J., &amp; Trick, L. M. (2009). Sympathetic magic and perceptions of randomness: The hot hand versus the gambler’s fallacy. Thinking &amp; Reasoning, 15(2), 117-137. https://doi.org/10.1080/13546780902847137 Sedley, W., Gander, P. E., Kumar, S., Kovach, C. K., Oya, H., Kawasaki, H., … &amp; Griffiths, T. D. (2016). Neural signatures of perceptual inference. elife, 5, e11476. https://doi.org/10.7554/eLife.11476.001 Taleb, N. N. (2008). The black swan. Penguin Books. Tenenbaum, J. B., Kemp, C., Griffiths, T. L., &amp; Goodman, N. D. (2011). How to grow a mind: Statistics, structure, and abstraction. Science, 331(6022), 1279-1285. https://doi.org/10.1126/science.1192788 Thompson, W. C., &amp; Schumann, E. L. (1987). Interpretation of statistical evidence in criminal trials: The prosecutor’s fallacy and the defense attorney’s fallacy. Law and Human Behavior, 11(3), 167-187. https://doi.org/10.1007/BF01044641 Tukey, J. W. (1962). The Future of Data Analysis. Annals of Mathematical Statistics, 33, 1-67. https://doi.org/10.1214/AOMS/1177704711 Tversky, A., &amp; Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124-1131. https://doi.org/10.1126/science.185.4157.1124 Visalli, A., Capizzi, M., Ambrosini, E., Kopp, B., &amp; Vallesi, A. (2021). Electroencephalographic correlates of temporal Bayesian belief updating and surprise. NeuroImage, 231, 117867. https://doi.org/10.1016/j.neuroimage.2021.117867 Wiese, W. (2014). Perceptual presence in the Kuhnian-Popperian Bayesian brain: a commentary on Anil K. Seth. In Open Mind. Open MIND. https://doi.org/10.25358/openscience-450 Williams, D. (2021). Epistemic Irrationality in the Bayesian Brain. The British Journal for the Philosophy of Science, 72(4). https://doi.org/10.1093/bjps/axz044 "],["appendix.html", "Appendix Applied Bayes theorem Applied Bayes theorem with unknown unconditional probability", " Appendix Applied Bayes theorem Suppose we have a jar containing 100 marbles, of which 30 are red, and 70 are blue. We want to know the probability that a randomly selected marble will be red, given that it is a small marble. We also know that 40 of the marbles in the jar are small, and 25 of those small marbles are red. Let \\(P(\\text{red})\\) be the probability that a randomly selected marble from the jar is red. Then, \\[ P(\\text{red}) = \\frac{30}{100} = \\frac{3}{10} = 0.3 \\] Let \\(P(\\text{small})\\) be the probability that a randomly selected marble from the jar is small. Then, \\[ P(\\text{small}) = \\frac{40}{100} = \\frac{2}{5} = 0.4 \\] Let \\(P(\\text{red and small})\\) be the probability that a randomly selected marble from the jar is both red and small. Then, \\[ P(\\text{red and small}) = \\frac{25}{100} = \\frac{1}{4} = 0.25 \\] Then, using Bayes’ theorem, we can calculate the probability that a small marble is red as follows: \\[ P(\\text{red} | \\text{small}) = \\frac{P(\\text{red and small}) \\times P(\\text{small})}{P(\\text{red})} = \\frac{0.25 \\times 0.4}{0.3} \\approx 0.33 \\] Therefore, the probability that a small marble is red is approximately 33%. Applied Bayes theorem with unknown unconditional probability Suppose we want to calculate the probability that it will rain tomorrow, given that the weather forecast predicts a 60% chance of rain. We also know that the weather forecast is correct 70% of the time and that it rains on 20% of days. Let \\(P(A)\\) be the probability that it will rain. Let \\(P(B)\\) be the probability that the weather forecast predicts rain. Let \\(P(A | B)\\) be the probability that it will rain, given that the forecast predicts rain. Let \\(P(B | A)\\) be the probability that the forecast predicts rain, given that it will rain. Let \\(P(B | ¬A)\\) be the probability that the forecast predicts rain, given that it will not rain. Then, \\[ \\begin{aligned} P(\\text{rain tomorrow} | \\text{forecast predicts rain}) &amp;= \\frac{0.7 \\times 0.2}{0.7 \\times 0.2 + (1 - 0.7) \\times (1 - 0.2)} \\\\ &amp;= \\frac{0.267}{0.267 + 0.06} = \\frac{0.267}{0.327} \\approx 0.814 \\end{aligned} \\] Therefore, the probability that it will rain tomorrow, given that the forecast predicts rain is approximately 81.4%. "]]
